{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50667381-2acb-4f1e-8056-2bfc5db12a2e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'venv (Python 3.11.8)' requires the ipykernel package.\n",
      "\u001b[1;31mInstall 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/Users/benjaminneuendank/Uni/Code/venv/bin/python -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers import BitsAndBytesConfig\n",
    "import random\n",
    "import re\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4865892-4dd0-498b-a255-489f5f69ba93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n"
     ]
    }
   ],
   "source": [
    "model_name_stutter = \"LeoLM/leo-hessianai-70b\"\n",
    "model_name_normal = \"LeoLM/leo-hessianai-13b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8be196",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Lade Tokenizer...\")\n",
    "tokenizer_stutter = AutoTokenizer.from_pretrained(model_name_stutter)\n",
    "tokenizer_normal = AutoTokenizer.from_pretrained(model_name_normal)\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "\n",
    "print(\"Lade Stotternde KI (70B)...\")\n",
    "model_stutter = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name_stutter,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=quantization_config,\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "\n",
    "print(\"Lade Normale KI (13B)...\")\n",
    "model_normal = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name_normal,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=quantization_config,\n",
    "    torch_dtype=torch.float16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f49ddf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_stutter(text, stutter_prob=0.15):\n",
    "    \"\"\"\n",
    "    Add stuttering to text\n",
    "    \"\"\"\n",
    "    words = text.split()\n",
    "    stuttered_text = []\n",
    "    \n",
    "    for word in words:\n",
    "        clean_word = re.sub(r'[^\\w]', '', word)\n",
    "\n",
    "        # Different stutter types\n",
    "        if len(clean_word) > 2 and random.random() < stutter_prob:\n",
    "            stutter_type = random.choice(['repeat', 'partial', 'block'])\n",
    "            \n",
    "            if stutter_type == 'repeat' and len(clean_word) > 1:\n",
    "                repeat_part = clean_word[:random.randint(1, min(2, len(clean_word)//2))]\n",
    "                stuttered_word = f\"{repeat_part}-{repeat_part}-{word}\"\n",
    "            elif stutter_type == 'partial':\n",
    "                if len(clean_word) >= 3:\n",
    "                    partial = clean_word[:random.randint(1, 2)]\n",
    "                    stuttered_word = f\"{partial}... {word}\"\n",
    "                else:\n",
    "                    stuttered_word = word\n",
    "            else:\n",
    "                stuttered_word = f\"... {word}\"\n",
    "            \n",
    "            stuttered_text.append(stuttered_word)\n",
    "        else:\n",
    "            stuttered_text.append(word)\n",
    "    \n",
    "    return ' '.join(stuttered_text)\n",
    "\n",
    "def remove_stutter(text):\n",
    "    \"\"\"\n",
    "    Remove stutter patterns from text to create clean version for normal AI\n",
    "    \"\"\"\n",
    "    # Remove repeated patterns like \"w-w-wort\" -> \"wort\"\n",
    "    text = re.sub(r'(\\w+)-\\1-\\1', r'\\1', text)\n",
    "    # Remove partial patterns like \"wo... wort\" -> \"wort\"\n",
    "    text = re.sub(r'(\\w+)\\.\\.\\.\\s*', '', text)\n",
    "    # Remove standalone \"...\"\n",
    "    text = re.sub(r'\\.\\.\\.', '', text)\n",
    "    # Remove single letter repetitions like \"w-wort\" -> \"wort\"\n",
    "    text = re.sub(r'(\\w)-\\1', r'\\1', text)\n",
    "    \n",
    "    # Clean up any extra spaces\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.strip()\n",
    "\n",
    "def limit_to_three_sentences(text):\n",
    "    \"\"\"\n",
    "    Limit text to maximum 3 sentences\n",
    "    \"\"\"\n",
    "    # Split into sentences\n",
    "    sentences = re.split(r'[.!?]+', text)\n",
    "    \n",
    "    # Filter empty sentences\n",
    "    sentences = [s.strip() for s in sentences if s.strip()]\n",
    "    \n",
    "    # Take max 3 sentences\n",
    "    if len(sentences) > 3:\n",
    "        sentences = sentences[:3]\n",
    "        limited_text = '. '.join(sentences) + '.'\n",
    "    else:\n",
    "        limited_text = text\n",
    "    \n",
    "    return limited_text\n",
    "\n",
    "def generate_stutter_response(prompt, max_new_tokens=120, temperature=0.7):\n",
    "    \"\"\"\n",
    "    Generate response with stuttering using 70B model\n",
    "    \"\"\"\n",
    "    formatted_prompt = f\"\"\"Führe ein natürliches Gespräch auf Deutsch über Kalifornien. Antworte nur auf Deutsch und stelle auch Fragen:\n",
    "\n",
    "{prompt}\n",
    "\n",
    "Antwort auf Deutsch:\"\"\"\n",
    "    \n",
    "    inputs = tokenizer_stutter(formatted_prompt, return_tensors=\"pt\")\n",
    "    inputs = {key: value.to(model_stutter.device) for key, value in inputs.items()}\n",
    "    \n",
    "    # Generate answer\n",
    "    with torch.no_grad():\n",
    "        outputs = model_stutter.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=temperature,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer_stutter.eos_token_id,\n",
    "            eos_token_id=tokenizer_stutter.eos_token_id,\n",
    "            top_p=0.9,\n",
    "            repetition_penalty=1.1,\n",
    "            no_repeat_ngram_size=2,\n",
    "            early_stopping=True\n",
    "        )\n",
    "    \n",
    "    full_text = tokenizer_stutter.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    if \"Antwort auf Deutsch:\" in full_text:\n",
    "        answer = full_text.split(\"Antwort auf Deutsch:\")[-1].strip()\n",
    "    else:\n",
    "        answer = full_text.replace(formatted_prompt, \"\").strip()\n",
    "    \n",
    "    answer = answer.split('\\n')[0]\n",
    "    answer = limit_to_three_sentences(answer)\n",
    "    \n",
    "    # Add stutter\n",
    "    answer = add_stutter(answer)\n",
    "    \n",
    "    return answer\n",
    "\n",
    "def generate_normal_response(prompt, max_new_tokens=120, temperature=0.7):\n",
    "    \"\"\"\n",
    "    Generate normal response using 13B model\n",
    "    \"\"\"\n",
    "    formatted_prompt = f\"\"\"Führe ein natürliches Gespräch auf Deutsch über Kalifornien. Antworte nur auf Deutsch in flüssigem, korrektem Deutsch:\n",
    "\n",
    "{prompt}\n",
    "\n",
    "Antwort auf Deutsch:\"\"\"\n",
    "    \n",
    "    inputs = tokenizer_normal(formatted_prompt, return_tensors=\"pt\")\n",
    "    inputs = {key: value.to(model_normal.device) for key, value in inputs.items()}\n",
    "    \n",
    "    # Generate answer\n",
    "    with torch.no_grad():\n",
    "        outputs = model_normal.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=temperature,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer_normal.eos_token_id,\n",
    "            eos_token_id=tokenizer_normal.eos_token_id,\n",
    "            top_p=0.9,\n",
    "            repetition_penalty=1.2,\n",
    "            no_repeat_ngram_size=3,\n",
    "            early_stopping=True\n",
    "        )\n",
    "    \n",
    "    full_text = tokenizer_normal.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    if \"Antwort auf Deutsch:\" in full_text:\n",
    "        answer = full_text.split(\"Antwort auf Deutsch:\")[-1].strip()\n",
    "    else:\n",
    "        answer = full_text.replace(formatted_prompt, \"\").strip()\n",
    "    \n",
    "    answer = answer.split('\\n')[0]\n",
    "    answer = limit_to_three_sentences(answer)\n",
    "    \n",
    "    return answer\n",
    "\n",
    "def simulate_typing(text, is_stutter=False):\n",
    "    \"\"\"\n",
    "    Simulate typing with different speeds\n",
    "    \"\"\"\n",
    "    words = text.split()\n",
    "    for i, word in enumerate(words):\n",
    "        print(word, end=\" \", flush=True)\n",
    "        \n",
    "        if is_stutter:\n",
    "            # Stuttering AI has longer irregular pauses\n",
    "            if random.random() < 0.4:\n",
    "                pause = random.uniform(0.2, 0.8)\n",
    "                time.sleep(pause)\n",
    "            elif random.random() < 0.1:\n",
    "                # Longer pauses for block stuttering\n",
    "                time.sleep(1.0)\n",
    "        else:\n",
    "            # Normal AI has shorter and more regular pauses\n",
    "            if random.random() < 0.2:\n",
    "                pause = random.uniform(0.1, 0.3)\n",
    "                time.sleep(pause)\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"KI UNTERHALTUNG: NATÜRLICHES GESPRÄCH ÜBER KALIFORNIEN\")\n",
    "print(\"=\"*70)\n",
    "print(\"Normale KI (13B)  |  Stotternde KI (70B)\\n\")\n",
    "\n",
    "conversation_history = []\n",
    "\n",
    "# Static opening message\n",
    "start_message = \"Hallo! Schön, mit dir zu sprechen. Ich war letztes Jahr in Kalifornien und war total begeistert. Warst du schon mal dort?\"\n",
    "current_topic = start_message\n",
    "\n",
    "print(f\"Normale KI: {start_message}\")\n",
    "conversation_history.append(f\"Normale KI: {start_message}\")\n",
    "time.sleep(2)\n",
    "\n",
    "# Number of conversation rounds (including goodbye)\n",
    "num_rounds = 8\n",
    "\n",
    "for round_num in range(num_rounds):\n",
    "    print(f\"\\n--- Runde {round_num + 1} ---\")\n",
    "    \n",
    "    print(f\"Stotternde KI: \", end=\"\", flush=True)\n",
    "    response_stutter = generate_stutter_response(current_topic)\n",
    "    simulate_typing(response_stutter, is_stutter=True)\n",
    "    conversation_history.append(f\"Stotternde KI: {response_stutter}\")\n",
    "    print()\n",
    "    time.sleep(1)\n",
    "\n",
    "    # Check for goodbye\n",
    "    if any(word in response_stutter.lower() for word in ['tschüss', 'auf wiedersehen', 'bis bald', 'ciao', 'bye']):\n",
    "        break\n",
    "    \n",
    "    # Remove stutter from the response before giving it to normal AI\n",
    "    clean_prompt_for_normal = remove_stutter(response_stutter)\n",
    "    \n",
    "    print(f\"Normale KI: \", end=\"\", flush=True)\n",
    "    response_normal = generate_normal_response(clean_prompt_for_normal)\n",
    "    simulate_typing(response_normal, is_stutter=False)\n",
    "    conversation_history.append(f\"Normale KI: {response_normal}\")\n",
    "    print()\n",
    "    time.sleep(1)\n",
    "    \n",
    "    # Check for goodbye\n",
    "    if any(word in response_normal.lower() for word in ['tschüss', 'auf wiedersehen', 'bis bald', 'ciao', 'bye']):\n",
    "        break\n",
    "    \n",
    "    # Update topic for next round\n",
    "    current_topic = response_normal\n",
    "\n",
    "# Add natural goodbye if not already done\n",
    "if not any(word in conversation_history[-1].lower() for word in ['tschüss', 'auf wiedersehen', 'bis bald', 'ciao', 'bye']):\n",
    "    print(f\"\\nStotternde KI: \", end=\"\", flush=True)\n",
    "    goodbye_stutter = \"Also... ich m-muss jetzt langsam los. War schön mit dir zu reden! Bis bald!\"\n",
    "    simulate_typing(goodbye_stutter, is_stutter=True)\n",
    "    conversation_history.append(f\"Stotternde KI: {goodbye_stutter}\")\n",
    "    print()\n",
    "    time.sleep(1)\n",
    "    \n",
    "    print(f\"Normale KI: \", end=\"\", flush=True)\n",
    "    goodbye_normal = \"Ja, mir hat das Gespräch auch viel Spaß gemacht! Pass auf dich auf und bis zum nächsten Mal!\"\n",
    "    simulate_typing(goodbye_normal, is_stutter=False)\n",
    "    conversation_history.append(f\"Normale KI: {goodbye_normal}\")\n",
    "    print()\n",
    "\n",
    "# Conversation summary\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"GESPRÄCHSZUSAMMENFASSUNG:\")\n",
    "print(\"=\"*70)\n",
    "for i, line in enumerate(conversation_history, 1):\n",
    "    speaker = \"Normale KI\" if \"Normale KI\" in line else \"Stotternde KI\"\n",
    "    message = line.split(\": \", 1)[1]\n",
    "    print(f\"{i:2d}. {speaker}: {message}\")\n",
    "\n",
    "print(f\"\\nGespräch erfolgreich beendet! Dauer: {len(conversation_history)} Nachrichten\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
